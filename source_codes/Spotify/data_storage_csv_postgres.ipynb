{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/osboxes/spark/spark-2.4.0-bin-hadoop2.7/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, levenshtein, to_timestamp, to_date\n",
    "#from pyspark.sql.functions import col, split, levenshtein, to_timestamp, date_format\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "#conf = pyspark.SparkConf().setAppName('appName').setMaster('spark://192.168.11.128:8080')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#spark.enableHiveSupport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def set_Schema(cols):\n",
    "    simpleSchema = []\n",
    "    \n",
    "    for i in cols: \n",
    "        simpleSchema.append(StructField(i[0],i[1])) \n",
    "        \n",
    "    return StructType(fields=simpleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, levenshtein, to_timestamp, to_date\n",
    "def persistToStagingDB(csv_file, table_name, table_schema):\n",
    "    \n",
    "    df = spark.read.csv(csv_file, header=True,encoding=\"UTF-8\", quote='\"',escape='\"', schema =table_schema) \n",
    "    #Other options that can come in handy during csv import are:\n",
    "    #multiLine=True,ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,encoding=\"UTF-8\",sep=',',maxColumns=2,inferSchema=True\n",
    "    \n",
    "    if table_name == 'spotify_top_200_weekly':\n",
    "\n",
    "        #split date columns into start and end\n",
    "        df = df.withColumn(\"start_date\", split(col(\"date_range\"), \"--\").getItem(0)).withColumn(\"end_date\", split(col(\"date_range\"), \"--\").getItem(1))\n",
    "        df = df.withColumn(\"start_date\", df[\"start_date\"].cast(DateType()))\n",
    "        df = df.withColumn(\"end_date\", df[\"end_date\"].cast(DateType()))\n",
    "   \n",
    "    elif table_name == 'songkick_uk_events':\n",
    "        df = df.withColumn(\"start_date\",to_date(\"start_date\",\"d/M/y\"))        \n",
    "        df = df.withColumn(\"start_time\",to_timestamp(\"start_time\", \"H:mm\"))\n",
    "\n",
    "    #Persit to Postgres DB - available after session closes\n",
    "    try:\n",
    "        df.write \\\n",
    "        .jdbc(\"jdbc:postgresql://osboxes:5432/gig_stg\", table_name,\n",
    "              properties={\"user\": \"postgres\", \"password\": \"postgres\"})\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember to always change the column name to lowercase\n",
    "# source: https://stackoverflow.com/questions/56145841/pandas-adding-double-quotes-on-column-names\n",
    "table_name = 'spotify_top_200_weekly'\n",
    "columns = [('week_position',IntegerType()),\n",
    "           ('track_name',StringType()), \n",
    "           ('artist', StringType()), \n",
    "           ('streams', FloatType()), \n",
    "           ('date_range', StringType()), \n",
    "           ('region', StringType()), \n",
    "           ('spotify_id', StringType())]\n",
    "simple_struct = set_Schema(columns)\n",
    "\n",
    "csv_file = '../datasets/spotify_top_200_weekly.csv'\n",
    "\n",
    "persistToStagingDB(csv_file, table_name, simple_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember to always change the column name to lowercase\n",
    "# source: https://stackoverflow.com/questions/56145841/pandas-adding-double-quotes-on-column-names\n",
    "\n",
    "table_name = 'spotify_track_details'\n",
    "columns = [('track_spotify_id',StringType()),\n",
    "           ('artist_spotify_id',StringType()), \n",
    "           ('artist_mbid', StringType()), \n",
    "           ('artist_name', StringType()),\n",
    "           ('artist_fychart_name', StringType()),            \n",
    "           ('track_url', StringType()), \n",
    "           ('track_popularity', FloatType()), \n",
    "           ('track_duration_ms', IntegerType()),\n",
    "           ('track_is_local', BooleanType()), \n",
    "           ('album_id', StringType()), \n",
    "           ('album_track_number', IntegerType()), \n",
    "           ('album_release_date', StringType()), \n",
    "           ('album_type', StringType())\n",
    "          ]\n",
    "simple_struct = set_Schema(columns)\n",
    "csv_file = '../datasets/spotify_track_details.csv'\n",
    "\n",
    "#df = spark.read.csv(csv_file, header=True,encoding=\"UTF-8\", quote='\"',escape='\"', schema =simple_struct)\n",
    "\n",
    "persistToStagingDB(csv_file, table_name, simple_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = 'songkick_uk_venues'\n",
    "columns = [('venue_id', IntegerType()),\n",
    "           ('name', StringType()),\n",
    "           ('street', StringType()),\n",
    "           ('post_code', StringType()),\n",
    "           ('city', StringType()),\n",
    "           ('country', StringType()),\n",
    "           ('capacity', FloatType()),\n",
    "           ('website', StringType()),\n",
    "           ('phone', StringType())\n",
    "          ]\n",
    "simple_struct = set_Schema(columns)\n",
    "csv_file = '../datasets/ukVenues.csv'\n",
    "\n",
    "persistToStagingDB(csv_file, table_name, simple_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TimestampType\n",
    "table_name = 'songkick_uk_events'\n",
    "columns = [('event_id', IntegerType()),\n",
    "           ('event_type', StringType()),\n",
    "           ('uri', StringType()),\n",
    "           ('age_restriction', StringType()),\n",
    "           ('mbid', StringType()),\n",
    "           ('venue_id', IntegerType()),\n",
    "           ('start_date', StringType()),\n",
    "           ('start_time', StringType()),\n",
    "           ('country', StringType()),\n",
    "           ('flagged_as_ended', BooleanType())\n",
    "          ]\n",
    "simple_struct = set_Schema(columns)\n",
    "csv_file = '../datasets/ukEvents.csv'\n",
    "\n",
    "persistToStagingDB(csv_file, table_name, simple_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option to add to spark.read.csv to ensure string in quotes and commas are treated as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_weeklyTop200 = spark.read.csv('../datasets/spotify_top_200_weekly.csv', header=True, nullValue='', schema=simple_struct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weeklyTop200.filter(df_weeklyTop200.spotify_id == '5mZXWEH2eh8zMZGCxT5aW0').show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataframe to temporary and permanent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist temporarily, not available when session closes\n",
    "df_weeklyTop200.createOrReplaceTempView('spotify_top_200_weekly')\n",
    "spark.sql('select count(1) from spotify_top_200_weekly').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist to HIVE Metastore - this is available after the session closes using spark.read.parquet('spark-warehouse/weeklytop200/')\n",
    "df_weeklyTop200.write.mode('Overwrite').saveAsTable('spotify_top_200_weekly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('spark-warehouse/spotify_top_200_weekly/').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
