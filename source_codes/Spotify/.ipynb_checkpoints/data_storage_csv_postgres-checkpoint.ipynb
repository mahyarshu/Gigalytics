{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"SPARK_HOME\"] = \"/home/osboxes/spark/spark-2.4.0-bin-hadoop2.7/\"\n",
    "os.environ[\"SPARK_HOME\"] = \"D:\\SHU_Drive\\ADMP\\Software\\Apache_Spark\\spark-3.0.0-preview2-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, levenshtein, to_timestamp, to_date, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession#, HiveContext\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "#conf = conf.config(\"spark.sql.warehouse.dir\", '../../datawarehouse_test/').enableHiveSupport().getOrCreate()\n",
    "#conf = pyspark.SparkConf().setAppName('appName').setMaster('spark://192.168.11.128:8080')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"D:\\SHU_Drive\\ADMP\\Software\\Apache_Spark\\spark-3.0.0-preview2-bin-hadoop2.7\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession#, HiveContext\n",
    "\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "spark = (SparkSession\n",
    "                .builder\n",
    "                .appName('example-pyspark-read-and-write-from-hive')\n",
    "                .master('local')\n",
    "                .config( conf=SparkConf())\n",
    "                .enableHiveSupport()\n",
    "                .getOrCreate()\n",
    "                )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_Schema(cols):\n",
    "    simpleSchema = []\n",
    "    \n",
    "    for i in cols: \n",
    "        simpleSchema.append(StructField(i[0],i[1])) \n",
    "        \n",
    "    return StructType(fields=simpleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, levenshtein, to_timestamp, to_date, when, date_format\n",
    "\n",
    "def persistToStagingDB(csv_file, table_name, table_schema):\n",
    "    \n",
    "    if table_name == 'mb_all_artist':\n",
    "        df = spark.read.csv(csv_file, header=True,encoding=\"UTF-8\", quote='\"',escape='\"', inferSchema='true') \n",
    "    else:\n",
    "        df = spark.read.csv(csv_file, header=True,encoding=\"UTF-8\", quote='\"',escape='\"', schema =table_schema) \n",
    "    #Other options that can come in handy during csv import are:\n",
    "    #multiLine=True,ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,encoding=\"UTF-8\",sep=',',maxColumns=2,inferSchema=True\n",
    "    \n",
    "    if table_name == 'spotify_top_200_weekly':\n",
    "\n",
    "        #split date columns into start and end\n",
    "        df = df.withColumn(\"start_date\", split(col(\"date_range\"), \"--\").getItem(0)).withColumn(\"end_date\", split(col(\"date_range\"), \"--\").getItem(1))\n",
    "        df = df.withColumn(\"start_date\", df[\"start_date\"].cast(DateType()))\n",
    "        df = df.withColumn(\"end_date\", df[\"end_date\"].cast(DateType()))\n",
    "        df = df.withColumn('region', when(df[\"region\"] == 'gb', 'UK').otherwise(df['region']))\n",
    "\n",
    "   \n",
    "    elif table_name == 'songkick_uk_events':\n",
    "        df = df.dropDuplicates(subset = ['event_id', 'mbid'])\n",
    "        #df = df.withColumn(\"start_date\", date_format('start_date', 'mm/dd/yyyy'))\n",
    "        #df = df.withColumn(\"start_date\",to_date(\"start_date\",\"d/M/y\"))        \n",
    "        \n",
    "        #df = df.withColumn('start_date', cast(DateType()))\n",
    "        df = df.withColumn(\"start_date\",to_date(\"start_date\",\"dd/MM/yyyy\"))        \n",
    "        df = df.withColumn(\"start_time\",to_timestamp(\"start_time\"))\n",
    "        \n",
    "    #Persit to Postgres DB - available after session closes\n",
    "    try:\n",
    "        df.write \\\n",
    "       .jdbc(\"jdbc:postgresql://osboxes:5432/test_gigalytics\", table_name, mode=\"overwrite\",\n",
    "              properties={\"user\": \"postgres\", \"password\": \"postgres\"})\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        \n",
    "        if hasattr(e, 'message'):\n",
    "            print(e.message)\n",
    "        else:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activeArtists.csv\n",
    "#remember to always change the column name to lowercase\n",
    "# source: https://stackoverflow.com/questions/56145841/pandas-adding-double-quotes-on-column-names\n",
    "def load_mb_all_artist():\n",
    "    table_name = 'mb_all_artist'\n",
    "    columns = [('gid',StringType()),\n",
    "               ('name',StringType()), \n",
    "               ('type', DoubleType()), \n",
    "               ('gender', DoubleType())]\n",
    "    simple_struct = set_Schema(columns)\n",
    "\n",
    "    csv_file = '../../datasets/artists.csv'\n",
    "\n",
    "    return persistToStagingDB(csv_file, table_name, simple_struct)\n",
    "\n",
    "\n",
    "#remember to always change the column name to lowercase\n",
    "# source: https://stackoverflow.com/questions/56145841/pandas-adding-double-quotes-on-column-names\n",
    "def load_spotify_top_200_weekly():\n",
    "    table_name = 'spotify_top_200_weekly'\n",
    "    columns = [('week_position',IntegerType()),\n",
    "               ('track_name',StringType()), \n",
    "               ('artist', StringType()), \n",
    "               ('streams', FloatType()), \n",
    "               ('date_range', StringType()), \n",
    "               ('region', StringType()), \n",
    "               ('spotify_id', StringType())]\n",
    "    simple_struct = set_Schema(columns)\n",
    "\n",
    "    csv_file = '../../datasets/spotify_top_200_weekly.csv'\n",
    "\n",
    "    return persistToStagingDB(csv_file, table_name, simple_struct)\n",
    "\n",
    "#remember to always change the column name to lowercase\n",
    "# source: https://stackoverflow.com/questions/56145841/pandas-adding-double-quotes-on-column-names\n",
    "def load_spotify_track_details():\n",
    "    table_name = 'spotify_track_details'\n",
    "    columns = [('track_spotify_id',StringType()),\n",
    "               ('artist_spotify_id',StringType()), \n",
    "               ('artist_mbid', StringType()), \n",
    "               ('artist_name', StringType()),\n",
    "               ('artist_fychart_name', StringType()),            \n",
    "               ('track_url', StringType()), \n",
    "               ('track_popularity', FloatType()), \n",
    "               ('track_duration_ms', IntegerType()),\n",
    "               ('track_is_local', BooleanType()), \n",
    "               ('album_id', StringType()), \n",
    "               ('album_track_number', IntegerType()), \n",
    "               ('album_release_date', StringType()), \n",
    "               ('album_type', StringType())\n",
    "              ]\n",
    "    simple_struct = set_Schema(columns)\n",
    "    csv_file = '../../datasets/spotify_track_details.csv'\n",
    "\n",
    "#df = spark.read.csv(csv_file, header=True,encoding=\"UTF-8\", quote='\"',escape='\"', schema =simple_struct)\n",
    "\n",
    "    return persistToStagingDB(csv_file, table_name, simple_struct)\n",
    "\n",
    "def load_songkick_uk_venues():\n",
    "    table_name = 'songkick_uk_venues'\n",
    "    columns = [('venue_id', IntegerType()),\n",
    "               ('name', StringType()),\n",
    "               ('street', StringType()),\n",
    "               ('post_code', StringType()),\n",
    "               ('city', StringType()),\n",
    "               ('country', StringType()),\n",
    "               ('capacity', FloatType()),\n",
    "               ('website', StringType()),\n",
    "               ('phone', StringType())\n",
    "              ]\n",
    "    simple_struct = set_Schema(columns)\n",
    "    csv_file = '../../datasets/ukVenues.csv'\n",
    "\n",
    "    return persistToStagingDB(csv_file, table_name, simple_struct)\n",
    "\n",
    "#TimestampType\n",
    "def load_songkick_uk_events():\n",
    "    table_name = 'songkick_uk_events'\n",
    "    columns = [('event_id', IntegerType()),\n",
    "               ('event_name', StringType()),\n",
    "               ('event_type', StringType()),\n",
    "               ('popularity', DoubleType()),\n",
    "               ('uri', StringType()),\n",
    "               ('age_restriction', StringType()),\n",
    "               ('mbid', StringType()),\n",
    "               ('venue_id', DoubleType()),\n",
    "               ('start_date', StringType()),\n",
    "               ('start_time', StringType()),\n",
    "               ('country', StringType()),\n",
    "               ('flagged_as_ended', BooleanType())           \n",
    "               #,('popularity', DoubleType())\n",
    "              ]\n",
    "    simple_struct = set_Schema(columns)\n",
    "    csv_file = '../../datasets/ukEvents.csv'\n",
    "\n",
    "    return persistToStagingDB(csv_file, table_name, simple_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_songkick_uk_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_mb_all_artist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_Data('mb_all_artist')\n",
    "df.printSchema()\n",
    "#df.select(['start_time']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_Data('songkick_uk_events').show()\n",
    "df = load_Data('songkick_uk_events')\n",
    "df.select(['start_time']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Data(db_table):\n",
    "    df = spark.read \\\n",
    "        .jdbc(\"jdbc:postgresql://osboxes:5432/test_gigalytics\", db_table, \\\n",
    "              properties={\"user\": \"postgres\", \"password\": \"postgres\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../datasets/spotify_top_200_weekly.csv', header=True,encoding=\"UTF-8\", quote='\"',escape='\"', inferSchema='true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../../datasets/artists.csv', header=True,encoding=\"UTF-8\", quote='\"',escape='\"', inferSchema='true') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../../datasets/ukEvents.csv', header=True,encoding=\"UTF-8\", quote='\"',escape='\"', inferSchema='true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.dropDuplicates(subset = ['eventID', 'artistMBID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(subset = ['gid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../datasets/ukVenues.csv', header=True,encoding=\"UTF-8\", quote='\"',escape='\"', inferSchema='true') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist postgres dim and facts tables to datawarehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_DW(db_table):\n",
    "    df = spark.read \\\n",
    "        .jdbc(\"jdbc:postgresql://osboxes:5432/test_gigalytics\", db_table, \\\n",
    "              properties={\"user\": \"postgres\", \"password\": \"postgres\"})\n",
    "    df.write.mode('overwrite').saveAsTable('default.'+ db_table)\n",
    "   # df.write.mode(\"overwrite\").saveAsTable(\"deatemp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_DW('dw_fct_artist_popularity')\n",
    "populate_DW('dw_fct_events')\n",
    "populate_DW('dw_fct_events_venues')\n",
    "populate_DW('dw_fct_ukevent_start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = spark.sql(\"Show databases\")\n",
    "db = spark.sql(\"select * from dw_fct_artist_popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .jdbc(\"jdbc:postgresql://osboxes:5432/test_gigalytics\", 'dw_fct_events', \\\n",
    "              properties={\"user\": \"postgres\", \"password\": \"postgres\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp_dw_fct_events\")\n",
    "#spark.sql(\"DROP TABLE IF EXISTS dw_fct_events\")\n",
    "df.write.mode('overwrite').saveAsTable('dw_fct_events')\n",
    "\n",
    "#spark.sql(\"CREATE TABLE dw_fct_events STORED AS ORC AS SELECT * FROM temp_dw_fct_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE dw_fct_events AS SELECT * FROM temp_dw_fct_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option to add to spark.read.csv to ensure string in quotes and commas are treated as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_weeklyTop200 = spark.read.csv('../datasets/spotify_top_200_weekly.csv', header=True, nullValue='', schema=simple_struct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weeklyTop200.filter(df_weeklyTop200.spotify_id == '5mZXWEH2eh8zMZGCxT5aW0').show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist dataframe to temporary and permanent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist temporarily, not available when session closes\n",
    "df_weeklyTop200.createOrReplaceTempView('spotify_top_200_weekly')\n",
    "spark.sql('select count(1) from spotify_top_200_weekly').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist to HIVE Metastore - this is available after the session closes using spark.read.parquet('spark-warehouse/weeklytop200/')\n",
    "df_weeklyTop200.write.mode('Overwrite').saveAsTable('spotify_top_200_weekly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('spark-warehouse/spotify_top_200_weekly/').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
